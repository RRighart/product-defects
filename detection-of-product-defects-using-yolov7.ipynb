{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"background-color:darkblue;font-family:arial;font-size:250%;color:white;text-align:center;border-radius:16px 16px;\"><b>Detection of product defects using Yolov7</b></p>","metadata":{}},{"cell_type":"markdown","source":"### Ruthger Righart\n#### rrighart@googlemail.com","metadata":{}},{"cell_type":"markdown","source":"Product packaging defects can disturb the supply chain and lead to customer insatisfaction. A wide variety of packaging problems can occur in the production line such as label issues, bad seals, damage to jar lids, such as scratches, deformations and holes. Product defects could cause contamination of food or other substances. It is therefore crucial to detect them without delay.","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimg = Image.open('/kaggle/input/jarlids/p20.JPG')\ni1 = ImageDraw.Draw(img)\nfont = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 16, encoding=\"unic\")\n\ni1.text((8, 8), \"Examples of jar lid damage\", font=font, fill=(255, 255, 255))\nimg","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-25T14:54:53.611090Z","iopub.execute_input":"2023-01-25T14:54:53.611905Z","iopub.status.idle":"2023-01-25T14:54:53.929403Z","shell.execute_reply.started":"2023-01-25T14:54:53.611822Z","shell.execute_reply":"2023-01-25T14:54:53.928527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Computer vision can be used to detect such defects. It is particularly suited for this purpose since it is fast and automated (little human assistance is needed) <span style=\"color:blue\">[1]</span>.\n\nThe current notebook is experimental and can be regarded as proof of concept. It examines if jar lid defects can be detected using Yolov7. Yolo (You Only Look Once) is an algorithm that has been used successfully for object detection in various interesting domains, such as the inspection of metal surface defects <span style=\"color:blue\">[2]</span>.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Table\"><b>Table of contents:</b>\n<ul>\n<li><a href=\"#Installation-packages\">Installation packages</a></li>  \n<li><a href=\"#Image-annotations\">Image data and annotations</a></li>         \n<li><a href=\"#Data-preprocessing\">Data preprocessing</a></li>\n<li><a href=\"#Visualization\">Visualization and annotation boxes</a></li>\n<li><a href=\"#Bounding-box\">Bounding box size</a></li>\n<li><a href=\"#Class-distribution\">Class distribution</a></li>\n<li><a href=\"#Preparation-annotations\">Preparation of annotations</a></li>\n<li><a href=\"#Training\">Training</a></li>\n<li><a href=\"#Visualization-learning\">Visualization of learning curves</a></li>\n<li><a href=\"#Unseen-testdata\">Unseen testdata</a></li>\n<li><a href=\"#Closing-words\">Closing words</a></li>\n    \n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"There is an App where you can interact with the final model. You can upload your own images and see the predicted classes.\n\nüëâ https://huggingface.co/spaces/rrighart/product-defects","metadata":{}},{"cell_type":"markdown","source":"**Yolo**","metadata":{}},{"cell_type":"markdown","source":"In Yolo, object detection is framed as a regression problem where a single neural network predicts bounding boxes and class probabilities directly from the presented image files. Yolo sees the entire image during training so it implicitly encodes contextual information (unlike sliding window and region proposal-based techniques) <span style=\"color:blue\">[3]</span>.\n\nYolov7 outperforms other object detectors in terms of accuracy and speed. It uses model reparameterization and model scaling <span style=\"color:blue\">[4]</span>.","metadata":{}},{"cell_type":"markdown","source":"![](https://github.com/WongKinYiu/yolov7/raw/main/figure/performance.png)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T15:33:50.671570Z","iopub.execute_input":"2023-01-06T15:33:50.672123Z","iopub.status.idle":"2023-01-06T15:33:51.767114Z","shell.execute_reply.started":"2023-01-06T15:33:50.672071Z","shell.execute_reply":"2023-01-06T15:33:51.765628Z"}}},{"cell_type":"markdown","source":"<a id=\"Installation-packages\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Installation packages</b></p>","metadata":{}},{"cell_type":"markdown","source":"We start with installing the needed packages. Do not forget to set the accelerator in the right panel at GPU.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/WongKinYiu/yolov7 \n!cd yolov7 && wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-25T14:54:53.930762Z","iopub.execute_input":"2023-01-25T14:54:53.931754Z","iopub.status.idle":"2023-01-25T14:55:00.761133Z","shell.execute_reply.started":"2023-01-25T14:54:53.931716Z","shell.execute_reply":"2023-01-25T14:55:00.759971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install imagesize","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-25T14:55:00.762849Z","iopub.execute_input":"2023-01-25T14:55:00.763995Z","iopub.status.idle":"2023-01-25T14:55:11.381247Z","shell.execute_reply.started":"2023-01-25T14:55:00.763954Z","shell.execute_reply":"2023-01-25T14:55:11.380042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ptitprince","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-25T14:55:11.385183Z","iopub.execute_input":"2023-01-25T14:55:11.385581Z","iopub.status.idle":"2023-01-25T14:55:21.871582Z","shell.execute_reply.started":"2023-01-25T14:55:11.385549Z","shell.execute_reply":"2023-01-25T14:55:21.870334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following modules will be used:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport json\nimport sys\nimport os\nimport shutil\nfrom distutils.dir_util import copy_tree\nimport imagesize\nimport ptitprince as pt\nfrom shutil import copyfile\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-25T14:55:21.873395Z","iopub.execute_input":"2023-01-25T14:55:21.874161Z","iopub.status.idle":"2023-01-25T14:55:22.567194Z","shell.execute_reply.started":"2023-01-25T14:55:21.874118Z","shell.execute_reply":"2023-01-25T14:55:22.566205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yolov7 is setting the seed automatically at 0 for `random.seed`, `np.random.seed` and `init_torch_seeds` in the function init_seeds that can be found in `yolov7/utils/general.py`. Note however that complete reproducibility across algorithm releases and CPU/GPU is not garanteed.","metadata":{}},{"cell_type":"code","source":"!grep -w init_seeds 'yolov7/utils/general.py' -A 4 ","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:22.568514Z","iopub.execute_input":"2023-01-25T14:55:22.568867Z","iopub.status.idle":"2023-01-25T14:55:23.513698Z","shell.execute_reply.started":"2023-01-25T14:55:22.568833Z","shell.execute_reply":"2023-01-25T14:55:23.512497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Image-data\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Image data and annotations</b></p>","metadata":{}},{"cell_type":"markdown","source":"Photos were taken using a Panasonic camera (DMC-TZ7). All images were width 640 by height 480 pixels. Photos contained multiple glass jars with jar lids that were intact or damaged. Damage could be subtle or severe, and categories were holes, deformations or scratches. Since there was a lack of training data, these categories were merged to a single category. There are a variety of confounding factors that make it difficult for machine vision to detect defects, such as luminance variation, light reflection and color. Jars were photographed in different positions and were rotated randomly.\nTraining data consisted of 124 images and validation of 39 images. 5 unseen test images were added as well. Data were annotated using the VGG Image Annotator (VIA) <span style=\"color:blue\">[5]</span>. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Data-preprocessing\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Data preprocessing</b></p>","metadata":{}},{"cell_type":"markdown","source":"Running `prepare_annotations()`, first the pandas DataFrame containing the annotation data is loaded. \nA directory structure is created which will contain the image files and label data. Label data are bounding box data normalized between 0 and 1. Data is split into a train and validation set.","metadata":{}},{"cell_type":"code","source":"annots = '/kaggle/input/jarlids/jarlids_annots.csv'\nimagefiles = '/kaggle/input/jarlids/'\ndest = '/kaggle/working/dest/'\nresults = '/kaggle/working/results.txt'","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:23.515481Z","iopub.execute_input":"2023-01-25T14:55:23.516199Z","iopub.status.idle":"2023-01-25T14:55:23.522396Z","shell.execute_reply.started":"2023-01-25T14:55:23.516154Z","shell.execute_reply":"2023-01-25T14:55:23.521403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yfile='train.yaml'\ntr_start=1\ntr_end=125\nva_start=200\nva_end=239\nte_start=1\nte_end=6\nva_te='val.txt'","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:23.524305Z","iopub.execute_input":"2023-01-25T14:55:23.524754Z","iopub.status.idle":"2023-01-25T14:55:23.533054Z","shell.execute_reply.started":"2023-01-25T14:55:23.524720Z","shell.execute_reply":"2023-01-25T14:55:23.532022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class prepare_annotations():\n    def __init__(self, sourcedir=annots, imagesdir=imagefiles, destdir=dest, namedir=dest, yamlfile=yfile, filename='im1.jpg', trainstart=tr_start, trainend=tr_end, valstart=va_start, valend=va_end, teststart=te_start, testend=te_end, validationtest=va_te):\n        self.sourcedir = sourcedir # directory where annotation data in CSV format are.\n        self.imagesdir = imagesdir # directory where original images are.\n        self.destdir = destdir # directory where annotations should be put.\n        self.namedir = namedir # directory where files are read when Yolov is run; paths are in the .txt and .yaml\n        self.yamlfile = yamlfile # name given to the yamlfile \n        self.filename = filename # for ex. p1.jpg\n        self.trainstart = trainstart # where train data start\n        self.trainend = trainend # where train data end (not including)\n        self.valstart = valstart # where validation data start\n        self.valend = valend # where validation data end (not including)\n        self.teststart = teststart # where testdata start\n        self.testend = testend # where testdata end\n        self.validationtest = validationtest # name of file used in test, can be 'val.txt' or 'test.txt'\n        \n    def isize(self, imagesdir, filename):\n        \"\"\"\n        Measures image dimensions without loading the image, using the package imagesize.\n        imagesdir: path directory to the image files\n        filename: name of image file\n        \"\"\"\n        w, h = imagesize.get(os.path.join(imagesdir, filename))\n        return(w, h)\n        \n    def load_dataframe(self):\n        \"\"\"\n        Loads annotation data with bounding box information and performs preprocessing\n        \"\"\"\n        dat = pd.read_csv(self.sourcedir)\n        tr = ['p'+str(s)+'.JPG' for s in range(self.trainstart, self.trainend)]\n        va = ['p'+str(s)+'.JPG' for s in range(self.valstart, self.valend)]\n        te = ['t'+str(s)+'.JPG' for s in range(self.teststart, self.testend)]\n        for i,j in zip(['train', 'val', 'test'], [tr, va, te] ):\n            dat.loc[dat['filename'].isin(j), 'dataset'] = i\n        dat['region_attributes']= dat['region_attributes'].replace({'{}': 'None'})\n        dat = dat[~dat['region_attributes'].isin(['None'])].reset_index(drop=False)\n        dat['category_names'] = dat['region_attributes'].apply(lambda x: str(list(eval(x).values())[0]))\n        dat['category_codes'] = dat[['category_names']].apply(lambda x: pd.Categorical(x).codes)\n        dat['image_width'] = dat['filename'].apply(lambda x: self.isize(self.imagesdir, x)[0])\n        dat['image_height'] = dat['filename'].apply(lambda x: self.isize(self.imagesdir, x)[1])\n        dat['x_min'] = dat['region_shape_attributes'].apply(lambda x: eval(x)['x'])\n        dat['y_min'] = dat['region_shape_attributes'].apply(lambda x: eval(x)['y'])\n        dat['bb_width'] = dat['region_shape_attributes'].apply(lambda x: eval(x)['width'])\n        dat['bb_height'] = dat['region_shape_attributes'].apply(lambda x: eval(x)['height'])\n        dat['n_x_center'] = (((dat['x_min'] + dat['bb_width']) + dat['x_min']) / 2) / dat['image_width'] \n        dat['n_y_center'] = (((dat['y_min'] + dat['bb_height']) + dat['y_min']) / 2) / dat['image_height'] \n        dat['n_width'] = dat['bb_width'] / dat['image_width'] \n        dat['n_height'] = dat['bb_height'] / dat['image_height']\n        dat['color_cat'] = dat['category_names'].replace({'intact': 'green', 'damaged': 'red'})\n        dat = dat.reset_index(drop=True)   \n        return(dat)\n    \n    def make_dirstructure(self):\n        \"\"\"\n        Creates directory structure and copies image files\n        \"\"\"\n        try:\n            print('new directory tree prepared')\n            os.makedirs(self.destdir, exist_ok=True)\n            os.makedirs(os.path.join(self.destdir, 'images/'), exist_ok=True)\n            os.makedirs(os.path.join(self.destdir, 'labels/'), exist_ok=True)\n        except:\n            print('no new directory was made, probably already existing')\n                \n        dat = self.load_dataframe()\n        filenames = list(set(dat['filename']))\n        for f in filenames:\n            copyfile(os.path.join(self.imagesdir, f), os.path.join(self.destdir, 'images', f))\n                                                \n    def make_labels(self):\n        \"\"\"\n        Saves bounding box data for given filenames as txt file for each image.\n        \"\"\"\n        dat = self.load_dataframe()\n        print('length data:', len(dat))\n        print('Emptying labelfiles')\n        for i in list(set(dat['filename'])):\n            try:\n                os.remove(os.path.join(self.destdir, 'labels/', i[:-4]+'.txt'))\n            except:\n                pass\n\n        print('Collecting bounding boxes, saving them to:', os.path.join(self.destdir, 'labels/'))\n        for i in range(0,len(dat)):\n            try:\n                with open(os.path.join(self.destdir, 'labels/', dat['filename'][i][:-4]+'.txt'), \"a\") as f:\n                    print(dat['category_codes'][i], dat['n_x_center'][i], dat['n_y_center'][i], dat['n_width'][i], dat['n_height'][i], file=f)\n            except:\n                print('something went wrong at', i)\n\n    def fixed_train_val_split(self):\n        \"\"\"\n        Images are separated given index. Images that contained the same jarlids but photographed from a different angle were in the same split\n        \"\"\"\n        dat = self.load_dataframe()\n    \n        tr = ['p'+str(i)+'.JPG' for i in range(self.trainstart,self.trainend)]\n        va = ['p'+str(i)+'.JPG' for i in range(self.valstart, self.valend)]\n        te = ['t'+str(i)+'.JPG' for i in range(self.teststart, self.testend)]\n        print('Splitting train and validation data')\n        \n        try:\n            print(\"Removed previous files\")\n            os.remove('train.txt')\n            os.remove('val.txt')\n            os.remove('test.txt')\n        except:\n            print(\"No previous files train.txt, val.txt or test.txt were found\")\n        \n        with open(os.path.join(self.destdir, 'train.txt'), \"a\") as f:\n            for i in range(len(tr)):\n                print(os.path.join(self.namedir, 'images', tr[i]), file=f)   \n        with open(os.path.join(self.destdir, 'val.txt'), \"a\") as f:\n            for i in range(len(va)):\n                print(os.path.join(self.namedir, 'images', va[i]), file=f)   \n        with open(os.path.join(self.destdir, 'test.txt'), \"a\") as f:\n            for i in range(len(te)):\n                print(os.path.join(self.namedir, 'images', te[i]), file=f)   \n                \n    def yaml_file(self):\n        \"\"\"\n        *lb: labels such as 'cat', 'dog'\n        namedir: # name directory where Docker Yolov5 reads the files.\n        yamlfile: name of the design file \n        validationtest: requires val.txt or test.txt\n        \"\"\"\n        dat = self.load_dataframe()\n        lb = list(sorted(set(dat['category_names'])))\n  \n        with open(os.path.join(self.destdir, self.yamlfile), \"a\") as f:\n            print('yaml:', file=f)\n            print('names:', file=f)\n            for i in lb:\n                print('- ', i, file=f)\n            print('nc:', len(lb), file=f)\n            print('train: ', os.path.join(self.namedir, 'train.txt'), file=f)\n            print('val: ', os.path.join(self.namedir, self.validationtest), file=f)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:23.536097Z","iopub.execute_input":"2023-01-25T14:55:23.536409Z","iopub.status.idle":"2023-01-25T14:55:23.570642Z","shell.execute_reply.started":"2023-01-25T14:55:23.536384Z","shell.execute_reply":"2023-01-25T14:55:23.569550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = prepare_annotations().load_dataframe()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:23.575298Z","iopub.execute_input":"2023-01-25T14:55:23.576540Z","iopub.status.idle":"2023-01-25T14:55:25.833200Z","shell.execute_reply.started":"2023-01-25T14:55:23.576499Z","shell.execute_reply":"2023-01-25T14:55:25.832212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:25.834651Z","iopub.execute_input":"2023-01-25T14:55:25.834990Z","iopub.status.idle":"2023-01-25T14:55:25.863074Z","shell.execute_reply.started":"2023-01-25T14:55:25.834957Z","shell.execute_reply":"2023-01-25T14:55:25.861984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Visualization\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Visualization and annotation boxes</b></p>","metadata":{}},{"cell_type":"markdown","source":"Let us visualize the annotations. This can of course also be done with the VGG Image Annotator which was used for annotation <span style=\"color:blue\">[5]</span>.","metadata":{}},{"cell_type":"code","source":"def visualisation_annotations(dat, filedir, fname):\n    \"\"\"\n    dat: input pandas DataFrame\n    filedir: directory where image files can be found\n    fname: filename to be visualized, for ex 'p1.JPG'  \n    \"\"\"\n    im = Image.open(os.path.join(filedir, fname))\n    fig, ax = plt.subplots(figsize=(14, 20))\n    ax.imshow(im)\n    ndat = dat[dat['filename'] == fname].reset_index()\n        \n    for i in range(len(ndat)):    \n        xmin = ndat['x_min'][i]\n        ymin = ndat['y_min'][i]\n        w = ndat['bb_width'][i]\n        h = ndat['bb_height'][i]\n        color = ndat['color_cat'][i]\n        rect = patches.Rectangle((xmin, ymin), w, h, linewidth=2, edgecolor=color, facecolor='none') # takes x, y, width and height.\n        ax.add_patch(rect)\n        \n    mpatches_dat = pd.DataFrame(df[['category_names', 'color_cat']].value_counts().reset_index())\n    patch1 = mpatches.Patch(color=mpatches_dat['color_cat'][0], label=mpatches_dat['category_names'][0])\n    patch2 = mpatches.Patch(color=mpatches_dat['color_cat'][1], label=mpatches_dat['category_names'][1])\n    ax.legend(handles=[patch1, patch2])\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:25.864838Z","iopub.execute_input":"2023-01-25T14:55:25.865228Z","iopub.status.idle":"2023-01-25T14:55:25.874327Z","shell.execute_reply.started":"2023-01-25T14:55:25.865193Z","shell.execute_reply":"2023-01-25T14:55:25.873146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualisation_annotations(df, imagefiles, 'p15.JPG')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:25.875843Z","iopub.execute_input":"2023-01-25T14:55:25.876532Z","iopub.status.idle":"2023-01-25T14:55:26.547926Z","shell.execute_reply.started":"2023-01-25T14:55:25.876494Z","shell.execute_reply":"2023-01-25T14:55:26.546268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Bounding-box\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Bounding box size</b></p>","metadata":{}},{"cell_type":"markdown","source":"Bounding box size may be a confounding factor and could be predictive in the train and validation set, while it is not in the unseen testdata. As can be seen in the Raincloud plot below, bounding box width and height was similar for intact and damaged jar lids.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\n\npt.RainCloud(x = \"category_names\", y = \"bb_width\", data = df[df['dataset'] == 'train'], palette = \"Set2\", bw = .2,\n                 width_viol = .6, ax = ax, orient = \"h\")\n\nplt.title(\"Bounding box width between intact and damaged jarlids\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:26.549384Z","iopub.execute_input":"2023-01-25T14:55:26.550086Z","iopub.status.idle":"2023-01-25T14:55:26.855962Z","shell.execute_reply.started":"2023-01-25T14:55:26.550036Z","shell.execute_reply":"2023-01-25T14:55:26.850530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\n\npt.RainCloud(x = \"category_names\", y = \"bb_height\", data = df[df['dataset'] == 'train'], palette = \"Set2\", bw = .2,\n                 width_viol = .6, ax = ax, orient = \"h\")\n\nplt.title(\"Bounding box height between intact and damaged jarlids\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:26.857405Z","iopub.execute_input":"2023-01-25T14:55:26.858076Z","iopub.status.idle":"2023-01-25T14:55:27.122311Z","shell.execute_reply.started":"2023-01-25T14:55:26.858031Z","shell.execute_reply":"2023-01-25T14:55:27.121356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A scatterplot shows that the dimensions of the bounding box have in general similar aspect ratios.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[14,8])\nsns.scatterplot(x='bb_width', y='bb_height', data=df.loc[df['dataset'] == 'train'].reset_index(), hue='category_names', alpha=0.7, s=50, palette=dict(intact=\"#4c9409\", damaged=\"#d48c11\"))","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:27.123734Z","iopub.execute_input":"2023-01-25T14:55:27.124702Z","iopub.status.idle":"2023-01-25T14:55:27.481239Z","shell.execute_reply.started":"2023-01-25T14:55:27.124663Z","shell.execute_reply":"2023-01-25T14:55:27.480325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Class-distribution\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Class distribution</b></p>","metadata":{}},{"cell_type":"markdown","source":"There is a slight imbalance between intact and damaged jarlids.","metadata":{}},{"cell_type":"code","source":"def count_classes(dat, dataset):\n    \"\"\"\n    dat: input DataFrame\n    dataset: \"train\", \"val\", or \"test\"\n    \"\"\"\n    return(df.loc[df['dataset'] == dataset, \"category_names\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:27.482697Z","iopub.execute_input":"2023-01-25T14:55:27.483731Z","iopub.status.idle":"2023-01-25T14:55:27.489272Z","shell.execute_reply.started":"2023-01-25T14:55:27.483693Z","shell.execute_reply":"2023-01-25T14:55:27.487989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Class distribution for train data","metadata":{}},{"cell_type":"code","source":"count_classes(df, \"train\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:27.490925Z","iopub.execute_input":"2023-01-25T14:55:27.491599Z","iopub.status.idle":"2023-01-25T14:55:27.509735Z","shell.execute_reply.started":"2023-01-25T14:55:27.491561Z","shell.execute_reply":"2023-01-25T14:55:27.508656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Class distribution for validation data","metadata":{}},{"cell_type":"code","source":"count_classes(df, \"val\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:27.511337Z","iopub.execute_input":"2023-01-25T14:55:27.511789Z","iopub.status.idle":"2023-01-25T14:55:27.525415Z","shell.execute_reply.started":"2023-01-25T14:55:27.511753Z","shell.execute_reply":"2023-01-25T14:55:27.524478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Preparation-annotations\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Preparation of annotations</b></p>","metadata":{}},{"cell_type":"markdown","source":"We will continue below with the preparation of annotations in the format that is needed for Yolov7. First of all, we start by making a directory structure where the image files and annotations can be stored. ","metadata":{}},{"cell_type":"code","source":"prepare_annotations().make_dirstructure()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:27.526828Z","iopub.execute_input":"2023-01-25T14:55:27.529682Z","iopub.status.idle":"2023-01-25T14:55:29.202133Z","shell.execute_reply.started":"2023-01-25T14:55:27.529644Z","shell.execute_reply":"2023-01-25T14:55:29.201085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepare_annotations().make_labels()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-01-25T14:55:29.203741Z","iopub.execute_input":"2023-01-25T14:55:29.204106Z","iopub.status.idle":"2023-01-25T14:55:30.860099Z","shell.execute_reply.started":"2023-01-25T14:55:29.204068Z","shell.execute_reply":"2023-01-25T14:55:30.858990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepare_annotations().fixed_train_val_split()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:30.861774Z","iopub.execute_input":"2023-01-25T14:55:30.862382Z","iopub.status.idle":"2023-01-25T14:55:32.431660Z","shell.execute_reply.started":"2023-01-25T14:55:30.862342Z","shell.execute_reply":"2023-01-25T14:55:32.430504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepare_annotations().yaml_file()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:32.433184Z","iopub.execute_input":"2023-01-25T14:55:32.433560Z","iopub.status.idle":"2023-01-25T14:55:33.960798Z","shell.execute_reply.started":"2023-01-25T14:55:32.433522Z","shell.execute_reply":"2023-01-25T14:55:33.959756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the data structure is ready for training the object detection model.","metadata":{}},{"cell_type":"code","source":"cat /kaggle/working/dest/train.yaml","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:33.962179Z","iopub.execute_input":"2023-01-25T14:55:33.962593Z","iopub.status.idle":"2023-01-25T14:55:34.952173Z","shell.execute_reply.started":"2023-01-25T14:55:33.962553Z","shell.execute_reply":"2023-01-25T14:55:34.950933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will look at the annotation data for one image file. The first column displays the category (intact or damaged), the next four columns are the normalized x_center, normalized y_center, normalized width and normalized height of each bounding box.","metadata":{}},{"cell_type":"code","source":"cat '/kaggle/working/dest/labels/p1.txt'","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:34.954401Z","iopub.execute_input":"2023-01-25T14:55:34.954872Z","iopub.status.idle":"2023-01-25T14:55:35.892620Z","shell.execute_reply.started":"2023-01-25T14:55:34.954827Z","shell.execute_reply":"2023-01-25T14:55:35.891389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Training\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Training</b></p>","metadata":{}},{"cell_type":"markdown","source":"The model will be trained using data augmentation. In the file `hypownsettings.yaml`, scaling and mosaic are set at 0 since during experiments it turned out that these settings had a negative impact on training. Flip upsidedown is set at 0.5.","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_MODE\"]=\"offline\"","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:35.894477Z","iopub.execute_input":"2023-01-25T14:55:35.895235Z","iopub.status.idle":"2023-01-25T14:55:35.900880Z","shell.execute_reply.started":"2023-01-25T14:55:35.895193Z","shell.execute_reply":"2023-01-25T14:55:35.899653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp 'yolov7/cfg/training/yolov7.yaml' '/kaggle/working/yolov7_ad.yaml' ","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:35.902551Z","iopub.execute_input":"2023-01-25T14:55:35.903265Z","iopub.status.idle":"2023-01-25T14:55:36.850879Z","shell.execute_reply.started":"2023-01-25T14:55:35.903228Z","shell.execute_reply":"2023-01-25T14:55:36.849542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp 'yolov7/cfg/training/yolov7.yaml' '/kaggle/working/yolov7_ad.yaml' \nwith open(\"/kaggle/working/yolov7_ad.yaml\") as r:\n  text = r.read().replace(\"nc: 80\", \"nc: 2\")\nwith open(\"/kaggle/working/yolov7_ad.yaml\", \"w\") as w:\n  w.write(text)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:36.859074Z","iopub.execute_input":"2023-01-25T14:55:36.859399Z","iopub.status.idle":"2023-01-25T14:55:37.809614Z","shell.execute_reply.started":"2023-01-25T14:55:36.859368Z","shell.execute_reply":"2023-01-25T14:55:37.808028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head -5 '/kaggle/working/yolov7_ad.yaml'","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:37.812782Z","iopub.execute_input":"2023-01-25T14:55:37.813957Z","iopub.status.idle":"2023-01-25T14:55:38.773008Z","shell.execute_reply.started":"2023-01-25T14:55:37.813909Z","shell.execute_reply":"2023-01-25T14:55:38.771749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/yolov7/","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:38.774710Z","iopub.execute_input":"2023-01-25T14:55:38.775510Z","iopub.status.idle":"2023-01-25T14:55:38.784011Z","shell.execute_reply.started":"2023-01-25T14:55:38.775460Z","shell.execute_reply":"2023-01-25T14:55:38.782861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp 'data/hyp.scratch.custom.yaml' '/kaggle/working/hypownsettings.yaml' \nwith open(\"/kaggle/working/hypownsettings.yaml\") as r:\n  text = r.read().replace(\"mosaic: 1.0\", \"mosaic: 0.0\").replace(\"flipud: 0.0\", \"flipud: 0.5\").replace(\"scale: 0.5\", \"scale: 0.0\")\nwith open(\"/kaggle/working/hypownsettings.yaml\", \"w\") as w:\n  w.write(text)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:55:38.785562Z","iopub.execute_input":"2023-01-25T14:55:38.786461Z","iopub.status.idle":"2023-01-25T14:55:39.728881Z","shell.execute_reply.started":"2023-01-25T14:55:38.786400Z","shell.execute_reply":"2023-01-25T14:55:39.727554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will start training. The flag `--image-weights` is used to correct for class imbalance. Training will benefit from pretrained weights.\nThis will take some time. You can grab a coffee ‚òïÔ∏è or tea ü´ñ in the meanwhile. ","metadata":{}},{"cell_type":"code","source":"!python train.py --img 640 --batch 16 --epochs 100 --data /kaggle/working/dest/train.yaml --hyp \"/kaggle/working/hypownsettings.yaml\" --image-weights --cfg \"/kaggle/working/yolov7_ad.yaml\" --weights '/kaggle/working/yolov7.pt' --cache","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-25T14:55:39.730763Z","iopub.execute_input":"2023-01-25T14:55:39.731414Z","iopub.status.idle":"2023-01-25T14:59:26.126613Z","shell.execute_reply.started":"2023-01-25T14:55:39.731371Z","shell.execute_reply":"2023-01-25T14:59:26.125323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training gives various files including a `results.txt` file that will in the following line be copied to the Kaggle working space. ","metadata":{}},{"cell_type":"code","source":"!cp '/kaggle/working/yolov7/runs/train/exp/results.txt' '/kaggle/working/results.txt'\n!cp '/kaggle/working/yolov7/runs/train/exp/results.png' '/kaggle/working/results.png'","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:59:26.129910Z","iopub.execute_input":"2023-01-25T14:59:26.130280Z","iopub.status.idle":"2023-01-25T14:59:28.144695Z","shell.execute_reply.started":"2023-01-25T14:59:26.130245Z","shell.execute_reply":"2023-01-25T14:59:28.143286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp '/kaggle/working/yolov7/runs/train/exp/weights/best.pt' '/kaggle/working/y7-prdef.pt'","metadata":{"execution":{"iopub.status.busy":"2023-01-25T15:09:30.007793Z","iopub.execute_input":"2023-01-25T15:09:30.008238Z","iopub.status.idle":"2023-01-25T15:09:31.011518Z","shell.execute_reply.started":"2023-01-25T15:09:30.008205Z","shell.execute_reply":"2023-01-25T15:09:31.009967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Visualization-learning\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Visualization of the learning curves</b></p>","metadata":{}},{"cell_type":"markdown","source":"To evaluate the results for our purpose, we will look if the bounding boxes are well located around the jar lids and if the predicted class corresponds to the annotated class.\nTo start the visualization, we will need to replace multiple spaces with commas in the `results.txt` file, so we can display performance below.","metadata":{}},{"cell_type":"code","source":"column_names = ['empty', 'x1', 'x2', 'Box', 'Objectness', 'Classification', 'x3', 'x4', 'x5', 'Precision', 'Recall', 'mAP@0.5', 'mAP@0.5:0.95', 'val_Box', 'val_Obj', 'val_Class']","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:59:28.146940Z","iopub.execute_input":"2023-01-25T14:59:28.147363Z","iopub.status.idle":"2023-01-25T14:59:28.153376Z","shell.execute_reply.started":"2023-01-25T14:59:28.147320Z","shell.execute_reply":"2023-01-25T14:59:28.152319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_spaces(inputtext, outputtext, colnames):\n    \"\"\"\n    removes spaces and \\n from results.txt file and returns data in dataframe\n    inputtext: for ex 'results.txt'\n    outputtext: for ex 'results.csv'\n    colnames: list with columnnames\n    \"\"\"\n    dat = pd.DataFrame(columns=colnames)\n\n    with open(inputtext) as f:\n        lines = f.readlines()\n    print('Length lines:', len(lines))        \n    \n    for i in range(len(lines)):\n        t1 = re.sub(' +', ', ', lines[i])\n        t2 = re.sub('\\n', '', t1)\n        t3 = t2.split(\",\")\n        dat.loc[i] = t3\n    dat.to_csv(outputtext)\n\n    return(dat)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:59:28.154978Z","iopub.execute_input":"2023-01-25T14:59:28.155658Z","iopub.status.idle":"2023-01-25T14:59:28.165546Z","shell.execute_reply.started":"2023-01-25T14:59:28.155623Z","shell.execute_reply":"2023-01-25T14:59:28.164602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_lcurves = text_spaces(inputtext=results, outputtext='/kaggle/working/results.txt', colnames=column_names)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:59:28.167087Z","iopub.execute_input":"2023-01-25T14:59:28.167758Z","iopub.status.idle":"2023-01-25T14:59:28.223995Z","shell.execute_reply.started":"2023-01-25T14:59:28.167721Z","shell.execute_reply":"2023-01-25T14:59:28.222820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nplt.figure(figsize = (14, 10))\n\nline1 = ['Box', 'Objectness', 'Classification', 'Precision', 'mAP@0.5']\nline2 = ['val_Box', 'val_Obj', 'val_Class', 'Recall', 'mAP@0.5:0.95']\ncol1 = ['blue', 'blue', 'blue', 'green', 'green']\ncol2 = ['red', 'red', 'red', 'orange', 'orange']\nylab = ['box_loss', 'obj_loss', 'cls_loss', 'recall and precision', 'mAP']\np1 = ['train', 'train', 'train', 'precision', 'mAP_0.5']\np2 = ['val', 'val', 'val', 'recall', 'mAP_0.5:0.95']\n\ndf_lcurves[line1] = df_lcurves[line1].astype(float)\ndf_lcurves[line2] = df_lcurves[line2].astype(float)\n\nfor i in range(len(line1)):\n    \n    ax = plt.subplot(3, 2, i+1)\n    ax.plot(df_lcurves[line1[i]], '.-', color=col1[i])\n    ax.plot(df_lcurves[line2[i]], '.-', color=col2[i])\n    ax.set_ylabel(ylab[i])\n    patch1 = mpatches.Patch(color=col1[i], label=p1[i])\n    patch2 = mpatches.Patch(color=col2[i], label=p2[i])\n    ax.legend(handles=[patch1, patch2])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:59:28.225823Z","iopub.execute_input":"2023-01-25T14:59:28.226183Z","iopub.status.idle":"2023-01-25T14:59:28.815025Z","shell.execute_reply.started":"2023-01-25T14:59:28.226149Z","shell.execute_reply":"2023-01-25T14:59:28.813352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mAP (mean Average Precision) is generally used in object detection to evaluate performance. The curves show an increasing trend even though they seem noisy. Other learning curves like bounding box loss are showing good performance as well.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Unseen-testdata\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Unseen testdata</b></p>","metadata":{}},{"cell_type":"markdown","source":"A few testdata were generated. To test more extensively, a greater variety of images should be used.","metadata":{}},{"cell_type":"code","source":"prepare_annotations(yamlfile='test_design.yaml', validationtest='test.txt').yaml_file()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:59:28.816646Z","iopub.execute_input":"2023-01-25T14:59:28.817310Z","iopub.status.idle":"2023-01-25T14:59:30.661597Z","shell.execute_reply.started":"2023-01-25T14:59:28.817271Z","shell.execute_reply":"2023-01-25T14:59:30.660493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python test.py --data /kaggle/working/dest/test_design.yaml --weights '/kaggle/working/yolov7/runs/train/exp/weights/best.pt' --augment","metadata":{"execution":{"iopub.status.busy":"2023-01-25T14:59:30.663377Z","iopub.execute_input":"2023-01-25T14:59:30.663767Z","iopub.status.idle":"2023-01-25T14:59:51.528654Z","shell.execute_reply.started":"2023-01-25T14:59:30.663731Z","shell.execute_reply":"2023-01-25T14:59:51.527332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results will be shown below. Class and bounding box location are quite alright.","metadata":{}},{"cell_type":"code","source":"Image.open('/kaggle/working/yolov7/runs/test/exp/test_batch0_labels.jpg')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-25T14:59:51.531695Z","iopub.execute_input":"2023-01-25T14:59:51.532622Z","iopub.status.idle":"2023-01-25T14:59:51.777678Z","shell.execute_reply.started":"2023-01-25T14:59:51.532580Z","shell.execute_reply":"2023-01-25T14:59:51.776788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Closing-words\"></a>\n<p style=\"background-color:darkblue;font-family:arial;font-size:120%;color:white;text-align:center;border-radius:2px 2px;\"><b>Closing words</b></p>","metadata":{}},{"cell_type":"markdown","source":"The learning curves are showing the desired trends and are going in the good direction. We should remember that the training dataset is rather small, and increasing the size will probably improve the model's generalizability to unseen testdata. This can then be tested for a larger variety of contexts.\nSeveral confounding factors, such as luminance variation, reflectivity, and color, may affect the performance and should be taken into account in order to deploy a machine vision model that can be used in production lines.\n\nPlease feel free to check out the model here:\nüëâ https://huggingface.co/spaces/rrighart/product-defects\n\n![product-defects](https://www.rrighart.com/uploads/8/3/7/7/83774724/gradioapp-product-defects_orig.png)","metadata":{}},{"cell_type":"markdown","source":"**References**","metadata":{}},{"cell_type":"markdown","source":"[1]. Using deep learning to detect defects in manufacturing. Materials, 13, 5755.\n\n[2]. Xu, Y., Zhang, K., Wang, L. (2021). Metal surface defect detection using modified YOLO. Algorithms.\n\n[3]. Redmon, J., Divvala, S., Girshick, R., Farhadi, A. (2016). You Only Look Once¬†: Unified, real-time object detection. https://arxiv.org/abs/1506.02640 \n\n[4]. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, https://arxiv.org/abs/2207.02696 \n\n[5]. VGG Image Annotator (VIA). https://www.robots.ox.ac.uk/~vgg/software/via/ .","metadata":{}},{"cell_type":"markdown","source":"If this notebook helped you, I'd very much appreciate your upvote üòá. \nPlease feel free to contact me at rrighart@googlemail.com or get in touch at [LinkedIn](https://www.linkedin.com/in/ruthger-righart).","metadata":{}},{"cell_type":"markdown","source":"<li><a href=\"#Table\">Table</a></li>","metadata":{}}]}